{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "  n_features: int\n",
    "  n_hidden: int\n",
    "\n",
    "  # We optimize n_instances models in a single training loop\n",
    "  # to let us sweep over sparsity or importance curves\n",
    "  # efficiently.\n",
    "\n",
    "  # We could potentially use torch.vmap instead.\n",
    "  n_instances: int\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               feature_probability: Optional[torch.Tensor] = None,\n",
    "               importance: Optional[torch.Tensor] = None,\n",
    "               device='cuda'):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    # they use the same transformation for the encoder and decoder\n",
    "    self.W = nn.Parameter(torch.empty((config.n_instances, config.n_features, config.n_hidden), device=device))\n",
    "    nn.init.xavier_normal_(self.W)\n",
    "    self.b_final = nn.Parameter(torch.zeros((config.n_instances, config.n_features), device=device))\n",
    "\n",
    "    if feature_probability is None:\n",
    "      feature_probability = torch.ones(())\n",
    "    self.feature_probability = feature_probability.to(device)\n",
    "    if importance is None:\n",
    "      importance = torch.ones(())\n",
    "    self.importance = importance.to(device)\n",
    "\n",
    "  def forward(self, features):\n",
    "    # features: [..., instance, n_features]\n",
    "    # W: [instance, n_features, n_hidden]\n",
    "    hidden = torch.einsum(\"...if,ifh->...ih\", features, self.W)\n",
    "    out = torch.einsum(\"...ih,ifh->...if\", hidden, self.W)\n",
    "    out = out + self.b_final\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "  def generate_batch(self, n_batch):\n",
    "    feat = torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device)\n",
    "    batch = torch.where(\n",
    "        torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,\n",
    "        feat,\n",
    "        torch.zeros((), device=self.W.device),\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    n_features=10,\n",
    "    n_hidden=2,\n",
    "    n_instances=1\n",
    ")\n",
    "model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
